---
title: "Practical Machine Learning Project"
output:
    html_document:
        theme: united
        highlight: zenburn
        cache: true
        self-contained: true
---

The goal of this project is to train a model on data provided by the [http://groupware.les.inf.puc-rio.br/har](Human activity Recognition Dataset). The dataset includes processed data from multiple sensors positioned on the body during the exercise. Participants were required to do the exercise the right way (class A) and the wrong way (classes B to E). The classes B to E represent the common failure people tend to do during this type of exercise.
The name of the feture labeling the class of exercise is named `classe`.

The validation data set is provided to check the accuracy of the trained model.

## Load and clean data

Manual inspection of the data revealed that NA values are in the form of "", "NA" and "#DIV/0". The training
and validation datasets are downloaded from url directly to help with the reproducibility of the analysis/training.

```{r, cache=TRUE, results='hide', echo=FALSE}
library(caret)
library(knitr)
library(RCurl)

trainingUrl <- "https://d396qusza40orc.cloudfront.net/predmachlearn/pml-training.csv"
data <- read.csv(textConnection(getURL(trainingUrl)), na.strings=c("","NA","#DIV/0!"))

validationUrl <- "https://d396qusza40orc.cloudfront.net/predmachlearn/pml-testing.csv"
validation <- read.csv(textConnection(getURL(validationUrl)), na.strings=c("","NA","#DIV/0!"))
```

I decided to remove all features containgin more the 90% of NA values.

```{r, results='hide'}
naCounts <- apply(data, 2, function(x) sum(is.na(x)))
columnsToIgnore <- c(names(data)[naCounts > 0.9 * nrow(data)], names(data)[1:7])
columnNumbersToIgnore <- which(names(data) %in% columnsToIgnore)

data <- data[, -columnNumbersToIgnore]
validation <- validation[, -columnNumbersToIgnore]
```

After removing the features with most NA values, there are `r ncol(data)-1` features left.

## Cross validation

The data is partitioned into training set concisting of 70% randomly selected rows and to training
set concisting of the residual rows. The `set.seed` is used for reproducibility. 

```{r, results='hide'}
set.seed(331)
inTrain  <- createDataPartition(y=data$classe, p=0.7, list=FALSE)
training <- data[inTrain,]
testing  <- data[-inTrain,]
```

## Training 

To speed up training process, I used the `doMC` library for parallel computing on posix OS. To do job in parallel
on windows, one should use `doSnow` package.

```{r, cache=TRUE, results='hide'}
library(doMC)
registerDoMC(cores=detectCores())
```

I trained two models. One with the *random forest* algorithm and the second with *stohastics gradient boosting*.
Random forest produced little smaller out of sample error (generalization error) than the stohastic gradient boosting. 

### Random Forest

```{r, cache=TRUE, results='hide'}
trainControlRf <- trainControl(method = "cv", number = 3, allowParallel = TRUE)
modFitRf <- train(classe~., data = training, method="rf", trControl = trainControlRf, prox=FALSE)
```

```{r, results='hide'}
rf.prediction <- predict(modFitRf, testing)
rf.confussion.matrix <- table(rf.prediction, testing$classe)
rf.computation.time <- modFitRf$times$everything[3]
rf.overall.accuracy <- mean(rf.prediction == testing$classe)
```

Training took `r round(rf.computation.time)` seconds. Overall (not by class) out of sample accuracy achieved was **`r sprintf("%.3f%%", rf.overall.accuracy*100)`**.

The confusion matrix for random forest is

```{r, echo=FALSE, results='asis'}
kable(rf.confussion.matrix)
```

### Stohastic Gradient Boosting Algorithm
The stohastic gradient boosting algorithm is known to produce the models that comapre in the accuracy with the random forest. In this case, the out of sample error rate was a little worse and the training time was almost 3 time as long as of
random forest.

```{r, cache=TRUE, results='hide'}
modFitBoost <- train(classe ~ ., method="gbm", data=training, verbose=F)
``` 

```{r, results='hide'}
boost.prediction <- predict(modFitBoost, testing)
boost.confussion.matrix <- table(boost.prediction, testing$classe)
boost.computation.time <- modFitBoost$times$everything[3]
boost.overall.accuracy <- mean(boost.prediction == testing$classe)
```

Training took `r round(boost.computation.time)` seconds. Overall in sample accuracy achieved was `r sprintf("%.3f%%", boost.overall.accuracy*100)`. The accuracy is somewhat lower than that of random forest model.

The confusion matrix for stohastic gradient boosting is

```{r, echo=FALSE, results='asis'}
kable(boost.confussion.matrix)
```

### Validating both models

Both models were good enough to correctly predict all of the validation set cases.

```{r, results='hide'}
# random forest
answers.rf <- as.character(predict(modFitRf, validation))

# boost
answers.boost <- as.character(predict(modFitBoost, validation))

answers <- as.data.frame(t(as.matrix(data.frame(random.forest=answers.boost, boost=answers.boost))))
names(answers) <- 1:20
```

Answers from both, random forest and boost model:

```{r, echo=FALSE, results='asis'}
kable(answers)
```


```{r, echo=FALSE}
# export answers to files
pml_write_files = function(x){
  n = length(x)
  for(i in 1:n){
    filename = paste0("answers/problem_id_",i,".txt")
    write.table(x[i],file=filename,quote=FALSE,row.names=FALSE,col.names=FALSE)
  }
}
pml_write_files(answers.rf)
```
