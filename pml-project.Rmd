---
title: "practical-machine-learning-project.Rmd"
output:
    html_document:
        theme: united
        highlight: zenburn
        cache: true
        cache.path: pml-project_cache
---

## Load and clean data

I decided to remove all columns containing more than 90% of NA values. I tested for non-informative (near zero variation) features in preprocessed data with but got none. 

```{r, cache=TRUE, results='hide'}
library(caret)
library(knitr)
library(RCurl)

trainingUrl <- "https://d396qusza40orc.cloudfront.net/predmachlearn/pml-training.csv"
data <- read.csv(textConnection(getURL(trainingUrl)), na.strings=c("","NA","#DIV/0!"))

validationUrl <- "https://d396qusza40orc.cloudfront.net/predmachlearn/pml-testing.csv"
validation <- read.csv(textConnection(getURL(validationUrl)), na.strings=c("","NA","#DIV/0!"))
```

## Cleaning data

I removed all rows containgin more the 90% of NA values.

```{r, results='hide'}
naCounts <- apply(data, 2, function(x) sum(is.na(x)))
columnsToIgnore <- c(names(data)[naCounts > 0.9 * nrow(data)], names(data)[1:7])
columnNumbersToIgnore <- which(names(data) %in% columnsToIgnore)

data <- data[, -columnNumbersToIgnore]
validation <- validation[, -columnNumbersToIgnore]
```

## Cross validation

The data is partitioned into training set concisting of 70% randomly selected rows and to training
set concisting of the residual rows. The `set.seed` is used for reproducibility. 

```{r}
set.seed(331)
inTrain <- createDataPartition(y=data$classe, p=0.7, list=FALSE)
training <- data[inTrain,]
testing <- data[-inTrain,]
```

## Exploratory data analysis

```{r}
features <- testing[, -53]
prComp <- prcomp((features - apply(features,2,mean))/apply(features,2,sd))
qplot(prComp$x[,1], prComp$x[,2], colour=testing$classe)

pc1 <-log10(sort(abs(prComp$rotation[,1]), decreasing=T))
pc2 <-log10(sort(abs(prComp$rotation[,2]), decreasing=T))

pc1 <- prComp$rotation[,1]
pc2 <- prComp$rotation[,2]

mostImportant <- names(sort(pc1^2, decreasing=T)[1:5])

# 5 most importnt features
qplot(abs(pc1)[mostImportant], abs(pc2)[mostImportant], colour=mostImportant) 
```

## Training 

To speed up training process, I used the `doMC` library for parallel computing on posix OS. To do job in parallel
on windows, one should use `doSnow` package.

```{r, cache=TRUE, results='hide'}
# https://class.coursera.org/predmachlearn-016/forum/thread?thread_id=31#post-148
registerDoMC(cores=detectCores())
```

I trained two models. One with the *random forest* algorithm and the second with *stohastics gradient boosting*.
Random forest produced little better out of sample error (generalization error). 

### Random Forest

```{r, cache=TRUE, results='hide'}
trainControlRf <- trainControl(method = "cv", number = 3, allowParallel = TRUE)
modFitRf <- train(classe~., data = training, method="rf", trControl = trainControlRf, prox=FALSE)
```

```{r, results='hide'}
rf.prediction <- predict(modFitRf, testing)
rf.confussion.matrix <- table(rf.prediction, testing$classe)
rf.computation.time <- modFitRf$times$everything[3]
#rf.overall.accuracy <- modFitRf$results[which(modFitRf$results$mtry==modFitRf$bestTune),2]

rf.overall.accuracy <- mean(rf.prediction == testing$classe)
```

Training took `r rf.computation.time` seconds. Overall (not by class) out of sample accuracy achieved was `r rf.overall.accuracy`.

The confusion matrix assesing testing data is

```{r, echo=FALSE, results='asis'}
kable(rf.confussion.matrix)
```

### Stohastics Gradient Boosting Algorithm
Do the training using Stohastics Gradient Boosting Algorithm

```{r, cache=TRUE, results='hide'}
modFitBoost <- train(classe ~ ., method="gbm", data=training, verbose=F)
``` 

```{r, cache=TRUE, results='hide'}
boost.prediction <- predict(modFitBoost, testing)
boost.confussion.matrix <- table(boost.prediction, testing$classe)
boost.computation.time <- modFitBoost$times$everything[3]
#boost.overall.accuracy <- modFitBoost$results[which(
#  modFitBoost$results$interaction.depth == modFitBoost$bestTune$interaction.depth & 
#  modFitBoost$results$n.trees == modFitBoost$bestTune$n.trees),4]

boost.overall.accuracy <- mean(boost.prediction == testing$classe)
```

Training took `r boost.computation.time` seconds. Overall in sample accuracy achieved was `r boost.overall.accuracy`.

The confusion matrix assesing testing data is

```{r, echo=FALSE, results='asis'}
kable(boost.confussion.matrix)
```

### Put the models to test with the validation data set

Both models were good enough to correctly predict all of the validation set cases.

```{r, results='hide'}
# random forest
answers.rf <- as.character(predict(modFitRf, validation))

# boost
answers.boost <- as.character(predict(modFitBoost, validation))

answers <- as.data.frame(t(as.matrix(data.frame(random.forest=answers.boost, boost=answers.boost))))
names(answers) <- 1:20
```

Answers from both, random forest and boost model:
```{r, echo=FALSE, results='asis'}
kable(answers)
```

### Export answers to files
```{r}
pml_write_files = function(x){
  n = length(x)
  for(i in 1:n){
    filename = paste0("answers/problem_id_",i,".txt")
    write.table(x[i],file=filename,quote=FALSE,row.names=FALSE,col.names=FALSE)
  }
}
pml_write_files(answers.rf)
```
